{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "#create glue session\n%idle_timeout 60\n%timeout 60\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 2\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nimport pyspark.sql.functions as f\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\nCurrent timeout is None minutes.\ntimeout has been set to 60 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 2\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 2\nIdle Timeout: 60\nTimeout: 60\nSession ID: f760cbd3-e5f8-4a1d-aa5d-b92008b19748\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\nWaiting for session f760cbd3-e5f8-4a1d-aa5d-b92008b19748 to get into ready status...\nSession f760cbd3-e5f8-4a1d-aa5d-b92008b19748 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#set bucket directory\nmart_bucket = 'cloud9-mart'",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#load schema from raw data catalog\ndf = glueContext.create_data_frame_from_catalog(database = \"cloud9_transformed\", table_name = \"tr_seoul_rescue\")\n\n#load schema from raw data catalog\ndf_district = glueContext.create_data_frame_from_catalog(database = \"cloud9_transformed\", table_name = \"tr_district_seoul\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# list loaded schema\n#df.printSchema()\n\n#print sample rows\n#df.show(3)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import *\n\n# 시간 데이터를 자리 수에 따라 처리하는 함수 정의\ndef format_time_column(column):\n    return when(length(column) == 2, concat(lit(\"00:00:\"), lpad(column.cast(\"string\"), 2, \"0\"))) \\\n        .when(length(column) == 3, concat(lit(\"00:\"), lpad((column / 100).cast(\"int\").cast(\"string\"), 2, \"0\"), lit(\":\"), lpad((column % 100).cast(\"int\").cast(\"string\"), 2, \"0\"))) \\\n        .when(length(column) == 4, concat(lit(\"00:\"), lpad((column / 100).cast(\"int\").cast(\"string\"), 2, \"0\"), lit(\":\"), lpad((column % 100).cast(\"int\").cast(\"string\"), 2, \"0\"))) \\\n        .when(length(column) == 5, concat(lpad((column / 10000).cast(\"int\").cast(\"string\"), 2, \"0\"), lit(\":\"), lpad(((column / 100) % 100).cast(\"int\").cast(\"string\"), 2, \"0\"), lit(\":\"), lpad((column % 100).cast(\"int\").cast(\"string\"), 2, \"0\"))) \\\n        .when(length(column) == 6, concat(lpad((column / 10000).cast(\"int\").cast(\"string\"), 2, \"0\"), lit(\":\"), lpad(((column / 100) % 100).cast(\"int\").cast(\"string\"), 2, \"0\"), lit(\":\"), lpad((column % 100).cast(\"int\").cast(\"string\"), 2, \"0\")))\n\n# 데이터프레임 변환\ndf_transformed = df \\\n    .withColumnRenamed(\"msfrtn_resc_reprt_no\", \"incident_report_id\") \\\n    .withColumnRenamed(\"dclr_ymd\", \"report_date\") \\\n    .withColumnRenamed(\"dsp_ymd\", \"dispatch_date\") \\\n    .withColumnRenamed(\"spt_arvl_ymd\", \"arrival_date\") \\\n    .withColumnRenamed(\"resc_cmptn_ymd\", \"completion_date\") \\\n    .withColumnRenamed(\"hmg_ymd\", \"return_date\") \\\n    .withColumnRenamed(\"sigungu_nm\", \"district_name\") \\\n    .withColumn(\"report_time\", col(\"dclr_tm\").cast(\"int\")) \\\n    .withColumn(\"dispatch_time\", col(\"dsp_tm\").cast(\"int\")) \\\n    .withColumn(\"arrival_time\", col(\"spt_arvl_tm\").cast(\"int\")) \\\n    .withColumn(\"completion_time\", col(\"resc_cmptn_tm\").cast(\"int\")) \\\n    .withColumn(\"return_time\", col(\"hmg_tm\").cast(\"int\")) \\\n    .withColumn(\"report_yr\", col(\"dclr_yr\").cast(\"int\")) \\\n    .withColumn(\"report_mnth\", col(\"dclr_mnth\").cast(\"int\")) \\\n    .withColumn(\"report_day\", col(\"dclr_day\").cast(\"int\")) \\\n    .withColumn(\n        \"report_timestamp\",\n        to_timestamp(\n            concat(\n                col(\"report_date\"), lit(\" \"),\n                format_time_column(col(\"report_time\"))\n            )\n        )\n    ) \\\n    .withColumn(\n        \"arrival_timestamp\",\n        to_timestamp(\n            concat(\n                col(\"arrival_date\"), lit(\" \"),\n                format_time_column(col(\"arrival_time\"))\n            )\n        )\n    ) \\\n    .withColumn(\n        \"time_difference\",\n        round((unix_timestamp(\"arrival_timestamp\") - unix_timestamp(\"report_timestamp\")) / 60, 2)\n    ) \\\n    .select(\n        \"incident_report_id\", \n        \"report_date\", \n        \"report_time\", \n        \"dispatch_date\", \n        \"dispatch_time\",\n        \"arrival_date\",\n        \"arrival_time\", \n        \"completion_date\", \n        \"completion_time\", \n        \"return_date\", \n        \"return_time\",\n        \"report_yr\",\n        \"report_mnth\",\n        \"report_day\",\n        \"district_name\",\n        \"report_timestamp\",\n        \"arrival_timestamp\",\n        \"time_difference\"\n    )\n\n# 결과 확인\n#df_transformed.printSchema()\n#df_transformed.show(5, truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 61,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# DISTINCT 처리된 district_name을 가져오기\ndf_district_distinct = df_district.select(\"district_name\").distinct()\n\n# 조인 수행\ndf_joined = df_transformed.join(\n    df_district_distinct,\n    df_transformed[\"district_name\"] == df_district_distinct[\"district_name\"],\n    how=\"inner\"\n).select(\n    df_transformed[\"incident_report_id\"], \n    df_transformed[\"report_date\"], \n    df_transformed[\"report_time\"], \n    df_transformed[\"dispatch_date\"], \n    df_transformed[\"dispatch_time\"],\n    df_transformed[\"arrival_date\"],\n    df_transformed[\"arrival_time\"], \n    df_transformed[\"completion_date\"], \n    df_transformed[\"completion_time\"], \n    df_transformed[\"return_date\"], \n    df_transformed[\"return_time\"],\n    df_transformed[\"report_yr\"],\n    df_transformed[\"report_mnth\"],\n    df_transformed[\"report_day\"],\n    df_transformed[\"report_timestamp\"],\n    df_transformed[\"arrival_timestamp\"],\n    df_transformed[\"time_difference\"]\n)\n\ndf_joined = df_joined.withColumn(\n    \"report_date\",\n    when(col(\"report_date\") < \"1900-01-01\", None).otherwise(col(\"report_date\"))\n).withColumn(\n    \"dispatch_date\",\n    when(col(\"dispatch_date\") < \"1900-01-01\", None).otherwise(col(\"dispatch_date\"))\n).withColumn(\n    \"arrival_date\",\n    when(col(\"arrival_date\") < \"1900-01-01\", None).otherwise(col(\"arrival_date\"))\n)\n\n\n# 결과 확인\n#df_joined.show(5, truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 62,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------------------+-----------+-----------+-------------+-------------+------------+------------+---------------+---------------+-----------+-----------+---------+-----------+----------+-------------------+-------------------+---------------+\n|incident_report_id|report_date|report_time|dispatch_date|dispatch_time|arrival_date|arrival_time|completion_date|completion_time|return_date|return_time|report_yr|report_mnth|report_day|report_timestamp   |arrival_timestamp  |time_difference|\n+------------------+-----------+-----------+-------------+-------------+------------+------------+---------------+---------------+-----------+-----------+---------+-----------+----------+-------------------+-------------------+---------------+\n|20181117507S01524 |2018-05-17 |47         |2018-05-17   |200          |2018-05-17  |400         |2018-05-17     |2000           |2018-05-17 |3000       |2018     |5          |17        |2018-05-17 00:00:47|2018-05-17 00:04:00|3.22           |\n|20181113509S01275 |2018-05-17 |312        |2018-05-17   |400          |2018-05-17  |800         |2018-05-17     |2800           |2018-05-17 |3500       |2018     |5          |17        |2018-05-17 00:03:12|2018-05-17 00:08:00|4.8            |\n|20181115509S01575 |2018-05-17 |330        |2018-05-17   |400          |2018-05-17  |900         |2018-05-17     |3200           |2018-05-17 |4000       |2018     |5          |17        |2018-05-17 00:03:30|2018-05-17 00:09:00|5.5            |\n|20181116103S00415 |2018-05-17 |444        |2018-05-17   |500          |2018-05-17  |1000        |2018-05-17     |2500           |2018-05-17 |4700       |2018     |5          |17        |2018-05-17 00:04:44|2018-05-17 00:10:00|5.27           |\n|20181107105S00385 |2018-05-17 |1059       |2018-05-17   |1300         |2018-05-17  |2000        |2018-05-17     |4000           |2018-05-17 |5000       |2018     |5          |17        |2018-05-17 00:10:59|2018-05-17 00:20:00|9.02           |\n+------------------+-----------+-----------+-------------+-------------+------------+------------+---------------+---------------+-----------+-----------+---------+-----------+----------+-------------------+-------------------+---------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Dynamic Partition Overwrite 설정\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\nspark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"LEGACY\")\n\n# Write transformed data to S3 with Dynamic Partition Overwrite\ndf_joined.write \\\n    .mode(\"overwrite\") \\\n    .format('parquet') \\\n    .partitionBy(\"report_yr\", \"report_mnth\", \"report_day\") \\\n    .save(f's3a://{mart_bucket}/incident_time_mart/')\n\n# Create tr_incident_time table if not exists\nspark.sql(\n    \"\"\"\n    CREATE EXTERNAL TABLE IF NOT EXISTS cloud9_mart.mt_incident_time (\n        `incident_report_id` STRING, \n        `report_date` DATE, \n        `report_time` INT, \n        `dispatch_date` DATE, \n        `dispatch_time` INT, \n        `arrival_date` DATE, \n        `arrival_time` INT, \n        `completion_date` DATE, \n        `completion_time` INT, \n        `return_date` DATE, \n        `return_time` INT,\n        `report_timestamp` TIMESTAMP,\n        `arrival_timestamp` TIMESTAMP,\n        `time_difference` DOUBLE\n    )\n    PARTITIONED BY ( \n        `report_yr` STRING, \n        `report_mnth` STRING, \n        `report_day` STRING\n    )\n    STORED AS PARQUET\n    LOCATION \n        's3://cloud9-mart/incident_time_mart/'\n    TBLPROPERTIES (\n        'classification' = 'parquet'\n    )\n    \"\"\"\n)\n\n# Add yesterday's partition\nfrom datetime import datetime, timedelta\n\n# 어제 날짜를 KST 기준으로 계산\nyesterday_kst = datetime.utcnow() + timedelta(hours=9) - timedelta(days=1)\n\n# 연도, 월, 일을 추출 (한 자리 수일 경우 앞의 0 제거)\nreport_yr = yesterday_kst.strftime('%Y')  # 연도는 그대로\nreport_mnth = yesterday_kst.strftime('%m').lstrip('0')  # 0 제거\nreport_day = yesterday_kst.strftime('%d').lstrip('0')   # 0 제거\n\n# Spark SQL로 파티션 추가\nspark.sql(\n    f\"\"\"\n    ALTER TABLE cloud9_mart.mt_incident_time \n    ADD IF NOT EXISTS \n    PARTITION (report_yr='{report_yr}', report_mnth='{report_mnth}', report_day='{report_day}') \n        LOCATION 's3a://cloud9-mart/incident_time_mart/report_yr={report_yr}/report_mnth={report_mnth}/report_day={report_day}/'\n    \"\"\"\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 63,
			"outputs": [
				{
					"name": "stdout",
					"text": "Py4JJavaError: An error occurred while calling o4147.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:279)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:193)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 109.0 failed 4 times, most recent failure: Lost task 11.3 in stage 109.0 (TID 1323) (172.35.235.97 executor 2): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:349)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:257)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading to Spark >= 3.0: \nwriting dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\ninto Parquet files can be dangerous, as the files may be read by Spark 2.x\nor legacy versions of Hive later, which uses a legacy hybrid calendar that\nis different from Spark 3.0+'s Proleptic Gregorian calendar. See more\ndetails in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\ndatetime values w.r.t. the calendar difference during writing, to get maximum\ninteroperability. Or set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"CORRECTED\" to write the datetime\nvalues as it is, if you are 100% sure that the written files will only be read by\nSpark 3.0+ or other systems that use Proleptic Gregorian calendar.\n\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:629)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:200)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:220)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:186)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:185)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:163)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:486)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:163)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:153)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:474)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.BaseDynamicPartitionDataWriter.writeRecord(FileFormatDataWriter.scala:328)\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataSingleWriter.write(FileFormatDataWriter.scala:370)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:332)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1550)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:339)\n\t... 9 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2229)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:246)\n\t... 46 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:349)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:257)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading to Spark >= 3.0: \nwriting dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\ninto Parquet files can be dangerous, as the files may be read by Spark 2.x\nor legacy versions of Hive later, which uses a legacy hybrid calendar that\nis different from Spark 3.0+'s Proleptic Gregorian calendar. See more\ndetails in SPARK-31404. You can set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"LEGACY\" to rebase the\ndatetime values w.r.t. the calendar difference during writing, to get maximum\ninteroperability. Or set \"spark.sql.parquet.datetimeRebaseModeInWrite\" to \"CORRECTED\" to write the datetime\nvalues as it is, if you are 100% sure that the written files will only be read by\nSpark 3.0+ or other systems that use Proleptic Gregorian calendar.\n\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:629)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:200)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createDateRebaseFuncInWrite$1(DataSourceUtils.scala:220)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:186)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:185)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:163)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:486)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:163)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:153)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:474)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.BaseDynamicPartitionDataWriter.writeRecord(FileFormatDataWriter.scala:328)\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataSingleWriter.write(FileFormatDataWriter.scala:370)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:332)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1550)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:339)\n\t... 9 more\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}