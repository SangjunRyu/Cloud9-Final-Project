{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "#create glue session\n%idle_timeout 60\n%timeout 60\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 2\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nimport pyspark.sql.functions as f\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\nCurrent timeout is None minutes.\ntimeout has been set to 60 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 2\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 2\nIdle Timeout: 60\nTimeout: 60\nSession ID: 9c034eb5-b2b4-4025-a653-699785fcdad6\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\nWaiting for session 9c034eb5-b2b4-4025-a653-699785fcdad6 to get into ready status...\nSession 9c034eb5-b2b4-4025-a653-699785fcdad6 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#set bucket directory\nraw_bucket = 'cloud9-batch-raw'\ntransformed_bucket = 'cloud9-transformed'",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#load schema from raw data catalog\ndf = glueContext.create_data_frame_from_catalog(database = \"cloud9_raw\", table_name = \"rw_fire_station\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# list loaded schema\n#df.printSchema()\n\n#print sample rows\n#df.show(3)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#write transformed data to S3\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import *\n\n# 'struct' 컬럼에서 'string'이 아닌 필드를 추출하고 필요한 타입으로 변환\ndf_transformed = df \\\n    .withColumn(\"latitude\", col(\"x-axis\")) \\\n    .withColumn(\"longitude\", col(\"y-axis\")) \\\n    .withColumn(\"year\", col(\"year\").cast(IntegerType()))\\\n    .select(\n        \"fire_station_name\", \n        \"fire_station_center_name\", \n        \"address\", \n        \"phone\", \n        \"fax\",\n        \"latitude\",\n        \"longitude\",\n        \"employee_count\",\n        \"year\"\n    )\n\n# 결과 확인\n#df_transformed.printSchema()\n#df_transformed.show(3)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- fire_station_name: string (nullable = true)\n |-- fire_station_center_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- phone: string (nullable = true)\n |-- fax: string (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- employee_count: long (nullable = true)\n |-- year: integer (nullable = true)\n\n+-----------------+------------------------+--------------------------------+------------+------------+-----------+-----------+--------------+----+\n|fire_station_name|fire_station_center_name|                         address|       phone|         fax|   latitude|  longitude|employee_count|year|\n+-----------------+------------------------+--------------------------------+------------+------------+-----------+-----------+--------------+----+\n| 서울소방재난본부|              강남소방서|서울특별시 강남구 테헤란로 62...|02-6981-7408| 02-556-2119|37.51027919|127.0668302|           356|2018|\n| 서울소방재난본부|              강동소방서| 서울특별시 강동구 성내로 39 ...|02-6981-7600|02-6981-7717|37.52945788|127.1253721|           266|2018|\n| 서울소방재난본부|              강북소방서| 서울특별시 강북구 한천로 911...|02-6946-0100|02-6946-0128|37.63305134|127.0381809|           228|2018|\n+-----------------+------------------------+--------------------------------+------------+------------+-----------+-----------+--------------+----+\nonly showing top 3 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Write transformed data to S3\ndf_transformed.write \\\n              .mode(\"overwrite\") \\\n              .format('parquet') \\\n              .partitionBy(\"year\") \\\n              .save(f's3a://{transformed_bucket}/fire_station_transformed/')\n\n# Create tr_fire_station table if not exists\nspark.sql(\n    \"\"\"\n    CREATE EXTERNAL TABLE IF NOT EXISTS cloud9_transformed.tr_fire_station (\n        `fire_station_name` STRING, \n        `fire_station_center_name` STRING, \n        `address` STRING, \n        `phone` STRING, \n        `fax` STRING, \n        `latitude` DOUBLE, \n        `longitude` DOUBLE, \n        `employee_count` BIGINT\n    )\n    PARTITIONED BY ( \n        `year` STRING\n    )\n    ROW FORMAT SERDE \n        'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \n    STORED AS INPUTFORMAT \n        'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' \n    OUTPUTFORMAT \n        'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n    LOCATION\n        's3://cloud9-transformed/fire_station_transformed/'\n    TBLPROPERTIES (\n        'classification' = 'parquet'\n    )\n    \"\"\"\n)\n\n# Add yesterday's partition\nfrom datetime import datetime, timedelta\n\nyesterday_kst = datetime.utcnow() + timedelta(hours=9)\nyear = yesterday_kst.strftime('%Y')\n\nspark.sql(\n    f\"\"\"\n    ALTER TABLE cloud9_transformed.tr_fire_station \n    ADD IF NOT EXISTS \n    PARTITION (year='{year}') \n        LOCATION 's3a://cloud9-transformed/fire_station_transformed/year={year}/'\n    \"\"\"\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		}
	]
}