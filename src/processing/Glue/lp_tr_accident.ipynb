{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "#create glue session\n%idle_timeout 30\n%timeout 30\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 2\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nimport pyspark.sql.functions as f\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Current idle_timeout is 30 minutes.\nidle_timeout has been set to 30 minutes.\nCurrent timeout is 30 minutes.\ntimeout has been set to 30 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 1\nSetting new number of workers to: 2\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 2\nIdle Timeout: 30\nTimeout: 30\nSession ID: eb6c02c4-8a28-403c-a731-d6cdf0fc1012\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\nWaiting for session eb6c02c4-8a28-403c-a731-d6cdf0fc1012 to get into ready status...\nSession eb6c02c4-8a28-403c-a731-d6cdf0fc1012 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from datetime import datetime, timedelta\n\n# Add today's partition in KST\ntoday_kst = datetime.utcnow() + timedelta(hours=9)\noccr_date = today_kst.strftime('%Y%m%d')  # Format as 'YYYYMMDD'\n\nspark.sql(\n    f\"\"\"\n    ALTER TABLE cloud9_transformed.tr_accident \n    ADD IF NOT EXISTS \n    PARTITION (occr_date='{occr_date}') \n        LOCATION 's3a://cloud9-transformed/accident_transformed/occr_date={occr_date}/'\n    \"\"\"\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		}
	]
}