{
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "#create glue session\n%idle_timeout 60\n%timeout 60\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 2\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nimport pyspark.sql.functions as F\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 2463911c-0b38-49a2-9534-b7d5bc87a9a6.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 2463911c-0b38-49a2-9534-b7d5bc87a9a6.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current timeout is None minutes.\ntimeout has been set to 60 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 2463911c-0b38-49a2-9534-b7d5bc87a9a6.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 5.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 2463911c-0b38-49a2-9534-b7d5bc87a9a6.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: None\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 2463911c-0b38-49a2-9534-b7d5bc87a9a6.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: None\nSetting new number of workers to: 2\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#set bucket directory\ntransformed_bucket = 'cloud9-transformed'\nmart_bucket = 'cloud9-mart'",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#load schema from transformed_bucket data catalog\ndf_district = glueContext.create_data_frame_from_catalog(database = \"cloud9_transformed\", table_name = \"tr_district_seoul\")\ndf_station = glueContext.create_data_frame_from_catalog(database = \"cloud9_mart\", table_name = \"mt_fire_station\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# list loaded schema\ndf_district.printSchema()\ndf_station.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- district_name: string (nullable = true)\n |-- dong_name: string (nullable = true)\n |-- population: integer (nullable = true)\n |-- area: double (nullable = true)\n |-- density: integer (nullable = true)\n |-- year: integer (nullable = true)\n\nroot\n |-- fire_station_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- phone: string (nullable = true)\n |-- fax: string (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- employee_count: long (nullable = true)\n |-- year: integer (nullable = true)\n |-- fire_station_center_name: string (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# address 칼럼에서 구 이름 추출하여 gu_nm 생성\nfrom pyspark.sql.functions import col, regexp_extract\n\ndf_station_gu_nm = df_station.withColumn(\"gu_nm\", regexp_extract(col(\"address\"), r\"([가-힣]+구)\", 0))\n\ndf_station_gu_nm_grouped = df_station_gu_nm.groupby(\"gu_nm\") \\\n    .agg(\n        F.count(\"gu_nm\").alias(\"subdistrict_count\"),\n        F.sum(\"employee_count\").alias(\"total_employee_count\")\n    )",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_station_gu_nm_grouped.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+-----------------+--------------------+\n|   gu_nm|subdistrict_count|total_employee_count|\n+--------+-----------------+--------------------+\n|영등포구|                6|                 284|\n|  성북구|                5|                 248|\n|  노원구|                6|                 275|\n|서대문구|                5|                 244|\n|  용산구|                6|                 252|\n|  광진구|                4|                 223|\n|  송파구|                7|                 324|\n|  동작구|                5|                 237|\n|  마포구|                7|                 314|\n|  관악구|                5|                 274|\n|  은평구|                5|                 257|\n|  중랑구|                5|                 245|\n|  강동구|                7|                 300|\n|  도봉구|                5|                 231|\n|  강북구|                5|                 241|\n|  금천구|                3|                 187|\n|  구로구|                7|                 303|\n|  서초구|                7|                 321|\n|  강서구|                7|                 318|\n|  성동구|                5|                 225|\n+--------+-----------------+--------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#df_district.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------+---------+----------+----+-------+----+\n|district_name|dong_name|population|area|density|year|\n+-------------+---------+----------+----+-------+----+\n|       강남구|  개포1동|     11907|1.27|   9376|2023|\n|       강남구|  개포2동|     41804|2.51|  16655|2023|\n|       강남구|  개포3동|     16894|1.24|  13624|2023|\n|       강남구|  개포4동|     23384|1.49|  15694|2023|\n|       강남구|  논현1동|     21759|1.25|  17407|2023|\n|       강남구|  논현2동|     20774|1.47|  14132|2023|\n|       강남구|  대치1동|     23934|0.79|  30296|2023|\n|       강남구|  대치2동|     38417| 2.0|  19209|2023|\n|       강남구|  대치4동|     18929|0.73|  25930|2023|\n|       강남구|  도곡1동|     20701|1.02|  20295|2023|\n|       강남구|  도곡2동|     32738|1.02|  32096|2023|\n|       강남구|  삼성1동|     12384|1.94|   6384|2023|\n|       강남구|  삼성2동|     30888|1.24|  24910|2023|\n|       강남구|   세곡동|     45724|6.36|   7189|2023|\n|       강남구|   수서동|     14090|1.43|   9853|2023|\n|       강남구|   신사동|     15681|1.89|   8297|2023|\n|       강남구| 압구정동|     25755|2.53|  10180|2023|\n|       강남구|  역삼1동|     35326|2.35|  15032|2023|\n|       강남구|  역삼2동|     36748|1.15|  31955|2023|\n|       강남구|  일원1동|     14711|0.92|  15990|2023|\n+-------------+---------+----------+----+-------+----+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# district_seoul 그룹화\nfrom pyspark.sql.functions import max, col\nfrom pyspark.sql.types import *\n\n# 최근 년도 데이터만 필터하기\nrecent_year = df_district.agg(max('year')).collect()[0][0]  # 'year' 칼럼의 최대값 구하기\nfiltered_df_district = df_district.filter((col('year') == recent_year) | col('year').isNull())\n\n# 그룹화\nfiltered_df_district_grouped = filtered_df_district.groupby(\"district_name\") \\\n    .agg(\n        F.count(\"district_name\").alias(\"gu_dong_count\"),\n        F.sum(\"population\").alias(\"total_population\"),\n        F.sum(\"area\").alias(\"total_area\"),  # 소수점 둘째 자리까지 포맷\n        F.sum(\"density\").alias(\"total_density\")\n    ) \\\n\n# 결과 출력\n#filtered_df_district.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "filtered_df_district_grouped.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 37,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- district_name: string (nullable = true)\n |-- gu_dong_count: integer (nullable = false)\n |-- total_population: double (nullable = true)\n |-- total_area: double (nullable = true)\n |-- total_density: integer (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import col\nfrom pyspark.sql.types import IntegerType, DoubleType\n\n# 'struct' 컬럼에서 'string'이 아닌 필드를 추출하고 필요한 타입으로 변환\nfiltered_df_district_grouped = filtered_df_district_grouped \\\n    .withColumn(\"district_name\", col(\"district_name\").cast(StringType())) \\\n    .withColumn(\"gu_dong_count\", col(\"gu_dong_count\").cast(IntegerType())) \\\n    .withColumn(\"total_population\", col(\"total_population\").cast(DoubleType())) \\\n    .withColumn(\"total_area\", col(\"total_area\").cast(DoubleType())) \\\n    .withColumn(\"total_density\", col(\"total_density\").cast(IntegerType())) \\\n    .select(\n        \"district_name\", \n        \"gu_dong_count\", \n        \"total_population\", \n        \"total_area\", \n        \"total_density\",\n    )",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# 조인하기\n\ndf_mart = filtered_df_district_grouped.join(df_station_gu_nm_grouped, filtered_df_district_grouped.district_name == df_station_gu_nm_grouped.gu_nm, \"outer\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#write mart data to S3\ndf_mart.write \\\n              .mode(\"overwrite\") \\\n              .format('parquet') \\\n              .partitionBy(\"gu_nm\") \\\n              .save(f's3a://{mart_bucket}/join_recent_district_seoul_fire_station_mart/')\n\n#create tr_fire_station table if not exists\nspark.sql(\n    \"\"\"\n    CREATE EXTERNAL TABLE IF NOT EXISTS cloud9_mart.mt_join_recent_district_seoul_fire_station (\n        `district_name` STRING,\n        `subdistrict_count` int, \n        `total_population` double, \n        `total_area` double, \n        `total_density` int, \n        `fire_station` int,\n        `firefighter` int\n    )\n    ROW FORMAT SERDE \n        'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \n    STORED AS INPUTFORMAT \n        'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' \n    OUTPUTFORMAT \n        'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n    LOCATION\n        's3://cloud9-mart/join_recent_district_seoul_fire_station_mart'\n    TBLPROPERTIES (\n        'classification' = 'parquet'\n    )\n    \"\"\"\n)\n\n#Load partitions\nspark.sql(\"MSCK REPAIR TABLE cloud9_mart.mt_fire_station\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		}
	]
}