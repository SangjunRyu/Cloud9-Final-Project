{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%idle_timeout 60\n%timeout 30\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 3\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nimport pyspark.sql.functions as f\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\nCurrent timeout is None minutes.\ntimeout has been set to 30 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 3\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 3\nIdle Timeout: 60\nTimeout: 30\nSession ID: a585cd45-42e7-4381-8458-610fba9fb974\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\nWaiting for session a585cd45-42e7-4381-8458-610fba9fb974 to get into ready status...\nSession a585cd45-42e7-4381-8458-610fba9fb974 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#set bucket directory\ntransformed_bucket = 'cloud9-transformed'",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#load schema from raw data catalog\ndf = glueContext.create_data_frame_from_catalog(database = \"cloud9_raw\", table_name = \"rw_seoul_rescue\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#df.printSchema()\n#df.show(3)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- msfrtn_resc_reprt_no: string (nullable = true)\n |-- acdnt_cause: string (nullable = true)\n |-- prcs_result_se_nm: string (nullable = true)\n |-- dclr_ymd: string (nullable = true)\n |-- dclr_tm: long (nullable = true)\n |-- season_se_nm: string (nullable = true)\n |-- qtr_se: long (nullable = true)\n |-- dclr_hour: long (nullable = true)\n |-- dclr_min: long (nullable = true)\n |-- daywk: string (nullable = true)\n |-- dsp_ymd: string (nullable = true)\n |-- dsp_tm: string (nullable = true)\n |-- dsp_yr: string (nullable = true)\n |-- dsp_mnth: string (nullable = true)\n |-- dsp_day: string (nullable = true)\n |-- dsp_hour: string (nullable = true)\n |-- dsp_min: string (nullable = true)\n |-- spt_arvl_ymd: string (nullable = true)\n |-- spt_arvl_tm: string (nullable = true)\n |-- spt_arvl_yr: string (nullable = true)\n |-- spt_arvl_mnth: string (nullable = true)\n |-- spt_arvl_day: string (nullable = true)\n |-- spt_arvl_hour: string (nullable = true)\n |-- spt_arvl_min: string (nullable = true)\n |-- resc_cmptn_ymd: string (nullable = true)\n |-- resc_cmptn_tm: string (nullable = true)\n |-- resc_cmptn_yr: string (nullable = true)\n |-- resc_cmptn_mnth: string (nullable = true)\n |-- resc_cmptn_day: string (nullable = true)\n |-- resc_cmptn_hour: string (nullable = true)\n |-- resc_cmptn_min: string (nullable = true)\n |-- hmg_ymd: string (nullable = true)\n |-- hmg_tm: string (nullable = true)\n |-- hmg_yr: string (nullable = true)\n |-- hmg_mnth: string (nullable = true)\n |-- hmg_day: string (nullable = true)\n |-- hmg_hour: string (nullable = true)\n |-- hmg_min: string (nullable = true)\n |-- sido_nm: string (nullable = true)\n |-- sigungu_nm: string (nullable = true)\n |-- emd_nm: string (nullable = true)\n |-- cty_frmvl_se_nm: string (nullable = true)\n |-- emd_se_nm: string (nullable = true)\n |-- gis_x_axis: double (nullable = true)\n |-- gis_y_axis: double (nullable = true)\n |-- acdnt_place_nm: string (nullable = true)\n |-- acdnt_place_detail_nm: string (nullable = true)\n |-- acdnt_cause_asort_nm: string (nullable = true)\n |-- frstt_nm: string (nullable = true)\n |-- ward_nm: string (nullable = true)\n |-- lfdau_nm: string (nullable = true)\n |-- time_unit_tmprt: string (nullable = true)\n |-- time_unit_rainqty: string (nullable = true)\n |-- time_unit_ws: string (nullable = true)\n |-- time_unit_wd: string (nullable = true)\n |-- time_unit_humidity: string (nullable = true)\n |-- time_unit_msnf: string (nullable = true)\n |-- time_unit: string (nullable = true)\n |-- dclr_yr: string (nullable = true)\n |-- dclr_mnth: string (nullable = true)\n |-- dclr_day: string (nullable = true)\n |-- spt_frstt_dist: double (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# Null 값이 있는 행 제거\ndf_dropped = df.dropna(subset=[\"spt_arvl_tm\"])\n\ndf_transformed = df_dropped \\\n    .withColumn(\"dclr_ymd\", to_date(regexp_replace(col(\"dclr_ymd\"), \"\\\\.0$\", \"\"), \"yyyyMMdd\")) \\\n    .withColumn(\"dclr_tm\", col(\"dclr_tm\").cast(\"int\")) \\\n    .withColumn(\"qtr_se\", col(\"qtr_se\").cast(\"int\")) \\\n    .withColumn(\"dclr_hour\", col(\"dclr_hour\").cast(\"int\")) \\\n    .withColumn(\"dclr_min\", col(\"dclr_min\").cast(\"int\")) \\\n    .withColumn(\"dsp_ymd\", to_date(regexp_replace(col(\"dsp_ymd\"), \"\\\\.0$\", \"\"), \"yyyyMMdd\")) \\\n    .withColumn(\"dsp_tm\", col(\"dsp_tm\").cast(\"int\")) \\\n    .withColumn(\"spt_arvl_ymd\", to_date(regexp_replace(col(\"spt_arvl_ymd\"), \"\\\\.0$\", \"\"), \"yyyyMMdd\")) \\\n    .withColumn(\"spt_arvl_tm\", col(\"spt_arvl_tm\").cast(\"int\")) \\\n    .withColumn(\"resc_cmptn_ymd\", to_date(regexp_replace(col(\"resc_cmptn_ymd\"), \"\\\\.0$\", \"\"), \"yyyyMMdd\")) \\\n    .withColumn(\"resc_cmptn_tm\", col(\"resc_cmptn_tm\").cast(\"int\")) \\\n    .withColumn(\"hmg_ymd\", to_date(regexp_replace(col(\"hmg_ymd\"), \"\\\\.0$\", \"\"), \"yyyyMMdd\")) \\\n    .withColumn(\"hmg_tm\", col(\"hmg_tm\").cast(\"int\")) \\\n    .withColumn(\"gis_x_axis\", col(\"gis_x_axis\")) \\\n    .withColumn(\"gis_y_axis\", col(\"gis_y_axis\")) \\\n    .withColumn(\"spt_frstt_dist\", col(\"spt_frstt_dist\"))\n\n# 결과 확인\n#df_transformed.printSchema()\n#df_transformed.show(3)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n\n# Write transformed data to S3\ndf_transformed.write \\\n              .mode(\"overwrite\") \\\n              .format('parquet') \\\n              .partitionBy(\"dclr_yr\", \"dclr_mnth\", \"dclr_day\") \\\n              .save(f's3a://{transformed_bucket}/seoul_rescue_transformed/')\n\n# Create tr_incident_report table if not exists\nspark.sql(\n    \"\"\"\n    CREATE EXTERNAL TABLE IF NOT EXISTS cloud9_transformed.tr_seoul_rescue (\n        `msfrtn_resc_reprt_no` STRING, \n        `acdnt_cause` STRING, \n        `prcs_result_se_nm` STRING, \n        `dclr_ymd` DATE, \n        `dclr_tm` INT, \n        `season_se_nm` STRING, \n        `qtr_se` INT, \n        `dclr_hour` INT, \n        `dclr_min` INT, \n        `daywk` STRING, \n        `dsp_ymd` DATE, \n        `dsp_tm` INT, \n        `dsp_yr` STRING, \n        `dsp_mnth` STRING, \n        `dsp_day` STRING, \n        `dsp_hour` STRING, \n        `dsp_min` STRING, \n        `spt_arvl_ymd` DATE, \n        `spt_arvl_tm` INT, \n        `spt_arvl_yr` STRING, \n        `spt_arvl_mnth` STRING, \n        `spt_arvl_day` STRING, \n        `spt_arvl_hour` STRING, \n        `spt_arvl_min` STRING, \n        `resc_cmptn_ymd` DATE, \n        `resc_cmptn_tm` INT, \n        `resc_cmptn_yr` STRING, \n        `resc_cmptn_mnth` STRING, \n        `resc_cmptn_day` STRING, \n        `resc_cmptn_hour` STRING, \n        `resc_cmptn_min` STRING, \n        `hmg_ymd` DATE, \n        `hmg_tm` INT, \n        `hmg_yr` STRING, \n        `hmg_mnth` STRING, \n        `hmg_day` STRING, \n        `hmg_hour` STRING, \n        `hmg_min` STRING, \n        `sido_nm` STRING, \n        `sigungu_nm` STRING, \n        `emd_nm` STRING, \n        `cty_frmvl_se_nm` STRING, \n        `emd_se_nm` STRING, \n        `gis_x_axis` DOUBLE, \n        `gis_y_axis` DOUBLE, \n        `spt_frstt_dist` DOUBLE, \n        `acdnt_place_nm` STRING, \n        `acdnt_place_detail_nm` STRING, \n        `acdnt_cause_asort_nm` STRING, \n        `frstt_nm` STRING, \n        `ward_nm` STRING, \n        `lfdau_nm` STRING, \n        `time_unit_tmprt` STRING, \n        `time_unit_rainqty` STRING, \n        `time_unit_ws` STRING, \n        `time_unit_wd` STRING, \n        `time_unit_humidity` STRING, \n        `time_unit_msnf` STRING, \n        `time_unit` STRING\n    )\n    PARTITIONED BY ( \n        `dclr_yr` STRING, \n        `dclr_mnth` STRING, \n        `dclr_day` STRING\n    )\n    STORED AS PARQUET\n    LOCATION \n        's3://cloud9-transformed/seoul_rescue_transformed/'\n    TBLPROPERTIES (\n        'classification' = 'parquet'\n    )\n    \"\"\"\n)\n\nfrom datetime import datetime, timedelta\n\n# 어제 날짜를 KST 시간으로 계산\nyesterday_kst = datetime.utcnow() + timedelta(hours=9) - timedelta(days=1)\n\n# 연도, 월, 일을 추출 (한 자리 수일 경우 앞의 0 제거)\ndclr_yr = yesterday_kst.strftime('%Y')  # 연도는 그대로\ndclr_mnth = yesterday_kst.strftime('%m').lstrip('0')  # 0 제거\ndclr_day = yesterday_kst.strftime('%d').lstrip('0')   # 0 제거\n\n# Spark SQL로 파티션 추가\nspark.sql(\n    f\"\"\"\n    ALTER TABLE cloud9_transformed.tr_seoul_rescue \n    ADD IF NOT EXISTS \n    PARTITION (dclr_yr='{dclr_yr}', dclr_mnth='{dclr_mnth}', dclr_day='{dclr_day}') \n        LOCATION 's3a://cloud9-transformed/seoul_rescue_transformed/dclr_yr={dclr_yr}/dclr_mnth={dclr_mnth}/dclr_day={dclr_day}/'\n    \"\"\"\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		}
	]
}